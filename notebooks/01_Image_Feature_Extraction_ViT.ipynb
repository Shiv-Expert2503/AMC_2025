{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21bccf014b2f4d2e847ead5ec2ea44ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_105e39f5ccde4905a0db08938101f0e1",
              "IPY_MODEL_5638c87e24d84593bca6fc3ae2ae529a",
              "IPY_MODEL_85103e7e7c184cd3ad46e12b8ec0e2c4"
            ],
            "layout": "IPY_MODEL_486fdf8d04fe49d7872293eb1d0d745c"
          }
        },
        "105e39f5ccde4905a0db08938101f0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f47e7a881b4ab099afc10ac6e096f1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e20012b3db534c24974fa7b6056cecfd",
            "value": "Processingâ€‡TRAIN:â€‡â€‡65%"
          }
        },
        "5638c87e24d84593bca6fc3ae2ae529a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f5ad8ffc9347649ba29a7627a86a43",
            "max": 2344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef4a717958164fa9a4f6d088e121c77d",
            "value": 1517
          }
        },
        "85103e7e7c184cd3ad46e12b8ec0e2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72c98a88a8d946898e6c7d980a8549e4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95c7362b3b684f8ab3ca529d7b396590",
            "value": "â€‡1517/2344â€‡[08:25&lt;04:34,â€‡â€‡3.01it/s]"
          }
        },
        "486fdf8d04fe49d7872293eb1d0d745c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100px"
          }
        },
        "31f47e7a881b4ab099afc10ac6e096f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e20012b3db534c24974fa7b6056cecfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f5ad8ffc9347649ba29a7627a86a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4a717958164fa9a4f6d088e121c77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72c98a88a8d946898e6c7d980a8549e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c7362b3b684f8ab3ca529d7b396590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yi8DuiuSSkj",
        "outputId": "72d64039-755a-408a-da8b-d996da1d82a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing required packages...\n",
            "This will take 2-3 minutes...\n",
            "\n",
            "\n",
            "âœ… All packages installed!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“¦ Installing required packages...\")\n",
        "print(\"This will take 2-3 minutes...\\n\")\n",
        "\n",
        "# Core ML libraries\n",
        "!pip install -q transformers torch torchvision\n",
        "\n",
        "# Data handling\n",
        "!pip install -q gcsfs pandas numpy pillow\n",
        "\n",
        "# Progress bars\n",
        "!pip install -q tqdm\n",
        "\n",
        "# Downgrade numpy if needed (for scipy compatibility)\n",
        "# !pip install -q 'numpy==1.26.4'\n",
        "\n",
        "print(\"\\nâœ… All packages installed!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: NOTEBOOK HEADER (Markdown Cell)\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "# Amazon ML Challenge 2025 - Phase 2: Vision Transformer Features\n",
        "\n",
        "**Objective:** Extract 768-dimensional visual features from product images using pre-trained Vision Transformer\n",
        "\n",
        "**Input:** 150,000 product images (75k train + 75k test) from GCS bucket\n",
        "\n",
        "**Output:**\n",
        "- `train_vit_features.npy` - Shape: (75000, 768) - Size: ~230 MB\n",
        "- `test_vit_features.npy` - Shape: (75000, 768) - Size: ~230 MB\n",
        "\n",
        "**Model:** google/vit-base-patch16-224 (86M parameters, Apache 2.0 license)\n",
        "\n",
        "**Timeline:** 3-4 hours total processing time\n",
        "\n",
        "**Status:** Phase 0 EDA Complete âœ… | Phase 2 In Progress ğŸ”„\n",
        "\n",
        "**Start Time:** Saturday, October 11, 2025 - 1:31 PM IST\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# GCS Configuration (from Phase 0 EDA)\n",
        "GCS_BUCKET = \"amazon_ml_challenge_data\"\n",
        "TRAIN_IMAGE_PATH = f\"gs://{GCS_BUCKET}/dataset/images/train\"\n",
        "TEST_IMAGE_PATH = f\"gs://{GCS_BUCKET}/dataset/images/test\"\n",
        "# TRAIN_CSV_PATH = f\"gs://{GCS_BUCKET}/dataset/train.csv\"\n",
        "# TEST_CSV_PATH = f\"gs://{GCS_BUCKET}/dataset/test.csv\"\n",
        "TRAIN_CSV_PATH = \"/content/train.csv\"\n",
        "TEST_CSV_PATH = \"/content/test.csv\"\n",
        "\n",
        "# Processing Configuration\n",
        "BATCH_SIZE = 32  # Process 32 images at once (optimal for T4 GPU)\n",
        "IMAGE_SIZE = 224  # ViT input size (224x224 pixels)\n",
        "FEATURE_DIM = 768  # ViT output dimension\n",
        "\n",
        "# Known Issues from EDA\n",
        "MISSING_TRAIN_SAMPLE_ID = 279285  # Image failed to download\n",
        "# Test missing ID unknown - will handle automatically\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ”¬ PHASE 2: VISION TRANSFORMER FEATURE EXTRACTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"ğŸ“¦ GCS Bucket: {GCS_BUCKET}\")\n",
        "print(f\"ğŸ–¼ï¸  Batch Size: {BATCH_SIZE} images\")\n",
        "print(f\"ğŸ“ Image Size: {IMAGE_SIZE}x{IMAGE_SIZE} pixels\")\n",
        "print(f\"ğŸ¯ Feature Dimension: {FEATURE_DIM}\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGn7opJoTMFT",
        "outputId": "69a3164d-434b-4bd4-b5bc-9facd04801ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ”¬ PHASE 2: VISION TRANSFORMER FEATURE EXTRACTION\n",
            "================================================================================\n",
            "ğŸ“¦ GCS Bucket: amazon_ml_challenge_data\n",
            "ğŸ–¼ï¸  Batch Size: 32 images\n",
            "ğŸ“ Image Size: 224x224 pixels\n",
            "ğŸ¯ Feature Dimension: 768\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: Import Libraries\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gcsfs\n",
        "from PIL import Image\n",
        "import io\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "print(\"âœ… Libraries imported!\\n\")\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"   âš ï¸  WARNING: No GPU detected!\")\n",
        "    print(\"   Go to Runtime â†’ Change runtime type â†’ Select GPU\")\n",
        "\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB2lRNxfTQCU",
        "outputId": "f50bed8e-1de1-46b6-8b70-6f84f468209b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported!\n",
            "\n",
            "ğŸ–¥ï¸  Device: cuda\n",
            "   GPU Name: Tesla T4\n",
            "   GPU Memory: 15.8 GB\n",
            "   CUDA Version: 12.6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "OjpYzdiTTsY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Connect to GCS and Load CSVs\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ğŸ”— Connecting to Google Cloud Storage...\")\n",
        "\n",
        "# Initialize GCS filesystem\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    test_files = fs.ls(f\"{GCS_BUCKET}/dataset\")\n",
        "    print(\"âœ… GCS connection successful!\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ GCS connection failed: {e}\")\n",
        "    print(\"Make sure you have access to the bucket!\\n\")\n",
        "\n",
        "# Load CSVs\n",
        "print(\"ğŸ“– Loading CSV files...\")\n",
        "\n",
        "with open(TRAIN_CSV_PATH, 'rb') as f:\n",
        "    df_train = pd.read_csv(f)\n",
        "print(f\"âœ… Train CSV loaded: {len(df_train):,} rows\")\n",
        "\n",
        "with open(TEST_CSV_PATH, 'rb') as f:\n",
        "    df_test = pd.read_csv(f)\n",
        "print(f\"âœ… Test CSV loaded: {len(df_test):,} rows\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk8qxPGBTXbl",
        "outputId": "18cbc128-19ed-4c70-c9c4-c05361d251ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Connecting to Google Cloud Storage...\n",
            "âœ… GCS connection successful!\n",
            "\n",
            "ğŸ“– Loading CSV files...\n",
            "âœ… Train CSV loaded: 75,000 rows\n",
            "âœ… Test CSV loaded: 75,000 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ============================================================================\n",
        "# CELL 6: Load Vision Transformer Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ğŸ¤– LOADING VISION TRANSFORMER MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(\"Model: google/vit-base-patch16-224\")\n",
        "print(\"Parameters: 86M\")\n",
        "print(\"License: Apache 2.0 âœ…\")\n",
        "print(\"Output: 768-dimensional feature vector\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"Downloading model weights (~340 MB)...\")\n",
        "print(\"This happens once, then cached...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load image processor (handles preprocessing)\n",
        "vit_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
        "print(\"âœ… Image processor loaded\")\n",
        "\n",
        "# Load model\n",
        "vit_model = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "print(\"âœ… Model architecture loaded\")\n",
        "\n",
        "# Move model to GPU\n",
        "vit_model = vit_model.to(device)\n",
        "vit_model.eval()  # Set to evaluation mode (no training)\n",
        "print(f\"âœ… Model moved to {device}\")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Vision Transformer ready! ({load_time:.1f}s)\")\n",
        "print(f\"   Input size: {IMAGE_SIZE}x{IMAGE_SIZE} pixels\")\n",
        "print(f\"   Output size: {FEATURE_DIM} features\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDzuKwqkT05-",
        "outputId": "607bfbf2-733f-4cde-edd3-a4455e09a5db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– LOADING VISION TRANSFORMER MODEL\n",
            "================================================================================\n",
            "Model: google/vit-base-patch16-224\n",
            "Parameters: 86M\n",
            "License: Apache 2.0 âœ…\n",
            "Output: 768-dimensional feature vector\n",
            "================================================================================\n",
            "\n",
            "Downloading model weights (~340 MB)...\n",
            "This happens once, then cached...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Image processor loaded\n",
            "âœ… Model architecture loaded\n",
            "âœ… Model moved to cuda\n",
            "\n",
            "âœ… Vision Transformer ready! (0.9s)\n",
            "   Input size: 224x224 pixels\n",
            "   Output size: 768 features\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Define Custom Dataset Class\n",
        "# ============================================================================\n",
        "\n",
        "class ProductImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading product images from GCS.\n",
        "\n",
        "    Handles:\n",
        "    - Loading images from GCS bucket\n",
        "    - Missing images (returns zero vector)\n",
        "    - Corrupted images (returns zero vector)\n",
        "    - Image preprocessing for ViT\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, image_base_path, fs, processor, image_size=224):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: DataFrame with sample_id column\n",
        "            image_base_path: GCS path to image folder\n",
        "            fs: gcsfs filesystem object\n",
        "            processor: ViT image processor\n",
        "            image_size: Target image size (224 for ViT)\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_base_path = image_base_path\n",
        "        self.fs = fs\n",
        "        self.processor = processor\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load and preprocess a single image.\n",
        "\n",
        "        Returns:\n",
        "            dict with:\n",
        "            - 'pixel_values': preprocessed image tensor\n",
        "            - 'sample_id': sample ID\n",
        "            - 'status': 'success' or 'missing'\n",
        "        \"\"\"\n",
        "        sample_id = self.df.iloc[idx]['sample_id']\n",
        "\n",
        "        # Construct image path\n",
        "        image_path = f\"{self.image_base_path}/{sample_id}.jpg\"\n",
        "\n",
        "        try:\n",
        "            # Load image from GCS\n",
        "            with self.fs.open(image_path, 'rb') as f:\n",
        "                image_data = f.read()\n",
        "\n",
        "            # Open with PIL\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "            # Convert to RGB (handles RGBA, grayscale, etc.)\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "            # Preprocess with ViT processor\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "            return {\n",
        "                'pixel_values': inputs['pixel_values'].squeeze(0),\n",
        "                'sample_id': sample_id,\n",
        "                'status': 'success'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Image not found or corrupted\n",
        "            # Return zero tensor (will become zero vector after ViT)\n",
        "            zero_image = torch.zeros(3, self.image_size, self.image_size)\n",
        "\n",
        "            return {\n",
        "                'pixel_values': zero_image,\n",
        "                'sample_id': sample_id,\n",
        "                'status': 'missing'\n",
        "            }\n",
        "\n",
        "print(\"âœ… Dataset class defined!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiKM6z5dVVDa",
        "outputId": "38b42f5f-d615-4580-abb6-8d52d2438959"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset class defined!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Feature Extraction Function\n",
        "# ============================================================================\n",
        "\n",
        "def extract_vit_features(dataloader, model, device, dataset_name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Extract ViT features for entire dataset using batched processing.\n",
        "\n",
        "    Args:\n",
        "        dataloader: PyTorch DataLoader\n",
        "        model: ViT model\n",
        "        device: cuda or cpu\n",
        "        dataset_name: Name for progress bar\n",
        "\n",
        "    Returns:\n",
        "        features: numpy array of shape (N, 768)\n",
        "        metadata: DataFrame with sample_id, status, processing_time\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_features = []\n",
        "    all_metadata = []\n",
        "\n",
        "    print(f\"ğŸ”¬ Extracting features for {dataset_name}...\")\n",
        "    print(f\"   Total batches: {len(dataloader)}\")\n",
        "    print(f\"   Batch size: {dataloader.batch_size}\")\n",
        "    print(f\"   Expected time: ~{len(dataloader) * 1.7 / 60:.0f} minutes\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation (faster)\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Processing {dataset_name}\", ncols=100)):\n",
        "            batch_start = time.time()\n",
        "\n",
        "            # Move batch to GPU\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            sample_ids = batch['sample_id']\n",
        "            statuses = batch['status']\n",
        "\n",
        "            # Forward pass through ViT\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "\n",
        "            # Extract pooled features (768-dim per image)\n",
        "            features = outputs.pooler_output  # Shape: (batch_size, 768)\n",
        "\n",
        "            # Move to CPU and convert to numpy\n",
        "            features_np = features.cpu().numpy()\n",
        "\n",
        "            # Store features\n",
        "            all_features.append(features_np)\n",
        "\n",
        "            # Store metadata\n",
        "            batch_time = time.time() - batch_start\n",
        "            for i in range(len(sample_ids)):\n",
        "                all_metadata.append({\n",
        "                    'sample_id': sample_ids[i].item(),\n",
        "                    'status': statuses[i],\n",
        "                    'batch_idx': batch_idx,\n",
        "                    'processing_time': batch_time / len(sample_ids)\n",
        "                })\n",
        "\n",
        "            # Clear GPU cache every 100 batches\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenate all features\n",
        "    features_array = np.vstack(all_features)\n",
        "\n",
        "    # Create metadata DataFrame\n",
        "    metadata_df = pd.DataFrame(all_metadata)\n",
        "\n",
        "    # Calculate statistics\n",
        "    elapsed_time = time.time() - start_time\n",
        "    images_per_sec = len(metadata_df) / elapsed_time\n",
        "\n",
        "    print(f\"\\nâœ… Feature extraction complete!\")\n",
        "    print(f\"   Features shape: {features_array.shape}\")\n",
        "    print(f\"   Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(f\"   Speed: {images_per_sec:.1f} images/second\")\n",
        "\n",
        "    # Count missing images\n",
        "    missing_count = (metadata_df['status'] == 'missing').sum()\n",
        "    success_count = (metadata_df['status'] == 'success').sum()\n",
        "\n",
        "    print(f\"\\n   Success: {success_count:,} ({100*success_count/len(metadata_df):.2f}%)\")\n",
        "    print(f\"   Missing: {missing_count:,} ({100*missing_count/len(metadata_df):.2f}%)\")\n",
        "\n",
        "    if missing_count > 0:\n",
        "        missing_ids = metadata_df[metadata_df['status'] == 'missing']['sample_id'].tolist()\n",
        "        if len(missing_ids) <= 10:\n",
        "            print(f\"   Missing sample IDs: {missing_ids}\")\n",
        "        else:\n",
        "            print(f\"   Missing sample IDs (first 10): {missing_ids[:10]}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return features_array, metadata_df\n",
        "\n",
        "print(\"âœ… Feature extraction function defined!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV_oAHzBVXne",
        "outputId": "88b7e9e1-cdb7-41cb-d8f4-b2dc72979af8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature extraction function defined!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Process TRAIN Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ PROCESSING TRAIN DATASET\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = ProductImageDataset(\n",
        "    df=df_train,\n",
        "    image_base_path=TRAIN_IMAGE_PATH,\n",
        "    fs=fs,\n",
        "    processor=vit_processor,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # Keep original order\n",
        "    num_workers=2,  # Parallel data loading\n",
        "    pin_memory=True  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š Train dataset prepared:\")\n",
        "print(f\"   Total images: {len(train_dataset):,}\")\n",
        "print(f\"   Batches: {len(train_dataloader):,}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\\n\")\n",
        "\n",
        "# Extract features\n",
        "train_features, train_metadata = extract_vit_features(\n",
        "    dataloader=train_dataloader,\n",
        "    model=vit_model,\n",
        "    device=device,\n",
        "    dataset_name=\"TRAIN\"\n",
        ")\n",
        "\n",
        "# Verify shape\n",
        "assert train_features.shape == (75000, 768), f\"Expected (75000, 768), got {train_features.shape}\"\n",
        "print(\"âœ… Shape verification passed!\\n\")\n",
        "\n",
        "# Feature statistics\n",
        "print(\"ğŸ“Š TRAIN FEATURE STATISTICS:\")\n",
        "print(f\"   Mean: {np.mean(train_features):.4f}\")\n",
        "print(f\"   Std: {np.std(train_features):.4f}\")\n",
        "print(f\"   Min: {np.min(train_features):.4f}\")\n",
        "print(f\"   Max: {np.max(train_features):.4f}\")\n",
        "print(f\"   NaN count: {np.isnan(train_features).sum()}\")\n",
        "print(f\"   Inf count: {np.isinf(train_features).sum()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "21bccf014b2f4d2e847ead5ec2ea44ed",
            "105e39f5ccde4905a0db08938101f0e1",
            "5638c87e24d84593bca6fc3ae2ae529a",
            "85103e7e7c184cd3ad46e12b8ec0e2c4",
            "486fdf8d04fe49d7872293eb1d0d745c",
            "31f47e7a881b4ab099afc10ac6e096f1",
            "e20012b3db534c24974fa7b6056cecfd",
            "56f5ad8ffc9347649ba29a7627a86a43",
            "ef4a717958164fa9a4f6d088e121c77d",
            "72c98a88a8d946898e6c7d980a8549e4",
            "95c7362b3b684f8ab3ca529d7b396590"
          ]
        },
        "id": "SZfeUPRBVaQm",
        "outputId": "6afeaa0c-bd4e-48e1-c6e1-af34da797f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ¯ PROCESSING TRAIN DATASET\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Train dataset prepared:\n",
            "   Total images: 75,000\n",
            "   Batches: 2,344\n",
            "   Batch size: 32\n",
            "\n",
            "ğŸ”¬ Extracting features for TRAIN...\n",
            "   Total batches: 2344\n",
            "   Batch size: 32\n",
            "   Expected time: ~66 minutes\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing TRAIN:   0%|                                                    | 0/2344 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21bccf014b2f4d2e847ead5ec2ea44ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Process TEST Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ PROCESSING TEST DATASET\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create dataset\n",
        "test_dataset = ProductImageDataset(\n",
        "    df=df_test,\n",
        "    image_base_path=TEST_IMAGE_PATH,\n",
        "    fs=fs,\n",
        "    processor=vit_processor,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š Test dataset prepared:\")\n",
        "print(f\"   Total images: {len(test_dataset):,}\")\n",
        "print(f\"   Batches: {len(test_dataloader):,}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\\n\")\n",
        "\n",
        "# Extract features\n",
        "test_features, test_metadata = extract_vit_features(\n",
        "    dataloader=test_dataloader,\n",
        "    model=vit_model,\n",
        "    device=device,\n",
        "    dataset_name=\"TEST\"\n",
        ")\n",
        "\n",
        "# Verify shape\n",
        "assert test_features.shape == (75000, 768), f\"Expected (75000, 768), got {test_features.shape}\"\n",
        "print(\"âœ… Shape verification passed!\\n\")\n",
        "\n",
        "# Feature statistics\n",
        "print(\"ğŸ“Š TEST FEATURE STATISTICS:\")\n",
        "print(f\"   Mean: {np.mean(test_features):.4f}\")\n",
        "print(f\"   Std: {np.std(test_features):.4f}\")\n",
        "print(f\"   Min: {np.min(test_features):.4f}\")\n",
        "print(f\"   Max: {np.max(test_features):.4f}\")\n",
        "print(f\"   NaN count: {np.isnan(test_features).sum()}\")\n",
        "print(f\"   Inf count: {np.isinf(test_features).sum()}\\n\")\n"
      ],
      "metadata": {
        "id": "kGpg6BV_ShSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Save Features to Local Storage\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ğŸ’¾ SAVING FEATURES...\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Save train features\n",
        "train_features_path = '/content/train_vit_features.npy'\n",
        "np.save(train_features_path, train_features)\n",
        "train_size_mb = os.path.getsize(train_features_path) / (1024**2)\n",
        "print(f\"âœ… Saved: train_vit_features.npy\")\n",
        "print(f\"   Location: {train_features_path}\")\n",
        "print(f\"   Size: {train_size_mb:.1f} MB\")\n",
        "print(f\"   Shape: {train_features.shape}\\n\")\n",
        "\n",
        "# Save test features\n",
        "test_features_path = '/content/test_vit_features.npy'\n",
        "np.save(test_features_path, test_features)\n",
        "test_size_mb = os.path.getsize(test_features_path) / (1024**2)\n",
        "print(f\"âœ… Saved: test_vit_features.npy\")\n",
        "print(f\"   Location: {test_features_path}\")\n",
        "print(f\"   Size: {test_size_mb:.1f} MB\")\n",
        "print(f\"   Shape: {test_features.shape}\\n\")\n",
        "\n",
        "# Save metadata\n",
        "train_metadata_path = '/content/train_vit_metadata.csv'\n",
        "train_metadata.to_csv(train_metadata_path, index=False)\n",
        "print(f\"âœ… Saved: train_vit_metadata.csv\")\n",
        "print(f\"   Location: {train_metadata_path}\\n\")\n",
        "\n",
        "test_metadata_path = '/content/test_vit_metadata.csv'\n",
        "test_metadata.to_csv(test_metadata_path, index=False)\n",
        "print(f\"âœ… Saved: test_vit_metadata.csv\")\n",
        "print(f\"   Location: {test_metadata_path}\\n\")\n",
        "\n",
        "print(f\"ğŸ“Š Total size: {train_size_mb + test_size_mb:.1f} MB\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 12: Mount Google Drive and Backup\n",
        "# ============================================================================\n",
        "\n",
        "print(\"â˜ï¸  BACKING UP TO GOOGLE DRIVE...\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directory\n",
        "drive_output_dir = '/content/drive/MyDrive/Amazon_ML_2025/phase2_image_features'\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ Output directory: {drive_output_dir}\\n\")\n",
        "\n",
        "# Copy files to Drive\n",
        "import shutil\n",
        "\n",
        "print(\"Copying files to Google Drive...\")\n",
        "\n",
        "# Copy train features\n",
        "shutil.copy(train_features_path, f\"{drive_output_dir}/train_vit_features.npy\")\n",
        "print(\"âœ… train_vit_features.npy copied\")\n",
        "\n",
        "# Copy test features\n",
        "shutil.copy(test_features_path, f\"{drive_output_dir}/test_vit_features.npy\")\n",
        "print(\"âœ… test_vit_features.npy copied\")\n",
        "\n",
        "# Copy metadata\n",
        "shutil.copy(train_metadata_path, f\"{drive_output_dir}/train_vit_metadata.csv\")\n",
        "print(\"âœ… train_vit_metadata.csv copied\")\n",
        "\n",
        "shutil.copy(test_metadata_path, f\"{drive_output_dir}/test_vit_metadata.csv\")\n",
        "print(\"âœ… test_vit_metadata.csv copied\")\n",
        "\n",
        "print(f\"\\nâœ… All files backed up to Google Drive!\")\n",
        "print(f\"   Location: {drive_output_dir}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 13: Final Summary and Next Steps\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ PHASE 2 COMPLETE - VISION TRANSFORMER FEATURES EXTRACTED!\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"ğŸ“Š SUMMARY:\")\n",
        "print(f\"   Train features: {train_features.shape} - {train_size_mb:.1f} MB\")\n",
        "print(f\"   Test features: {test_features.shape} - {test_size_mb:.1f} MB\")\n",
        "print(f\"   Total processing time: ~{(time.time() - start_time)/3600:.1f} hours\")\n",
        "print(f\"   Missing train images: {(train_metadata['status'] == 'missing').sum()}\")\n",
        "print(f\"   Missing test images: {(test_metadata['status'] == 'missing').sum()}\")\n",
        "\n",
        "print(\"\\nâœ… FILES SAVED:\")\n",
        "print(\"   Local (temporary):\")\n",
        "print(f\"      {train_features_path}\")\n",
        "print(f\"      {test_features_path}\")\n",
        "print(\"   Google Drive (permanent):\")\n",
        "print(f\"      {drive_output_dir}/train_vit_features.npy\")\n",
        "print(f\"      {drive_output_dir}/test_vit_features.npy\")\n",
        "\n",
        "print(\"\\nğŸ¯ NEXT STEPS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Phase 3: TEXT FEATURE EXTRACTION\")\n",
        "print(\"   A. Sentence Embeddings (384-dim)\")\n",
        "print(\"   B. Hand-Crafted REGEX Features (~20 features)\")\n",
        "print(\"   C. TF-IDF (Optional)\")\n",
        "print(\"\\nPhase 4: FEATURE COMBINATION\")\n",
        "print(\"   - Combine image + text features\")\n",
        "print(\"   - Apply log transform to target\")\n",
        "print(\"   - Create train/val split\")\n",
        "print(\"\\nPhase 5: MODEL TRAINING\")\n",
        "print(\"   - XGBoost + LightGBM + CatBoost\")\n",
        "print(\"   - Ensemble predictions\")\n",
        "print(\"   - Generate submission\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nğŸš€ Ready for Phase 3!\")\n",
        "print(\"â° Current time:\", time.strftime(\"%I:%M %p\"))\n",
        "print(\"ğŸ“… Date:\", time.strftime(\"%B %d, %Y\"))"
      ],
      "metadata": {
        "id": "OGB1TogVViS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}