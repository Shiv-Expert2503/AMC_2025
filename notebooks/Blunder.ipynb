{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WSiKNFfMfO9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load data\n",
        "# train = pd.read_csv('/mnt/sagemaker-nvme/feature_merge/train_CLEAN_FINAL.csv')\n",
        "# test = pd.read_csv('/mnt/sagemaker-nvme/feature_merge/test_CLEAN_FINAL.csv')\n",
        "\n",
        "# Also save sample\n",
        "X_train = pd.read_csv(\"/mnt/sagemaker-nvme/feature_merge/X_train_reduced.csv\")\n",
        "X_test = pd.read_csv(\"/mnt/sagemaker-nvme/feature_merge/X_test_reduced.csv\")\n",
        "y_train = pd.read_csv(\"/mnt/sagemaker-nvme/feature_merge/y_train.csv\")"
      ],
      "metadata": {
        "id": "522iQPfLMgjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
        "# print(f\"\\nTarget distribution:\")\n",
        "# print(train['price'].describe())\n",
        "# # print(f\"\\nLog_price distribution:\")\n",
        "# # print(train['log_price'].describe())\n",
        "# # train.drop(\"log_price\", inplace = True, axis=1)\n",
        "\n",
        "# # Check for problematic prices\n",
        "# print(f\"\\n‚ö†Ô∏è  Prices < 1: {(train['price'] < 1).sum()}\")\n",
        "# print(f\"‚ö†Ô∏è  Prices > 1000: {(train['price'] > 1000).sum()}\")"
      ],
      "metadata": {
        "id": "EavhEA0dMgg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_smape_safe_target(train):\n",
        "#     \"\"\"Fix extreme values that break SMAPE\"\"\"\n",
        "#     train = train.copy()\n",
        "\n",
        "#     # SMAPE breaks with near-zero denominators\n",
        "#     # Clip very low prices (0.5 minimum to prevent division issues)\n",
        "#     min_price = 0.5\n",
        "#     train['price'] = train['price'].clip(lower=min_price)\n",
        "\n",
        "#     # Winsorize extreme outliers (cap at 99.5th percentile)\n",
        "#     q995 = train['price'].quantile(0.995)\n",
        "#     train['price'] = train['price'].clip(upper=q995)\n",
        "\n",
        "#     # Recreate log target AFTER clipping\n",
        "#     train['log_price'] = np.log1p(train['price'])\n",
        "\n",
        "#     print(f\"‚úÖ Target range after SMAPE-safe clipping:\")\n",
        "#     print(f\"   Price: [{train['price'].min():.2f}, {train['price'].max():.2f}]\")\n",
        "#     print(f\"   Log: [{train['log_price'].min():.4f}, {train['log_price'].max():.4f}]\")\n",
        "\n",
        "#     return train\n",
        "\n",
        "# train = prepare_smape_safe_target(train)"
      ],
      "metadata": {
        "id": "dwI05W7BMgeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def reduce_feature_dimensionality(train, test):\n",
        "#     \"\"\"Reduce 2430 ‚Üí ~200-300 features to prevent overfitting\"\"\"\n",
        "\n",
        "#     # Separate feature groups\n",
        "#     sample_id = 'sample_id'\n",
        "#     target_cols = ['price', 'log_price']\n",
        "\n",
        "#     # Feature groups from your summary\n",
        "#     vit_cols = [c for c in train.columns if 'vit_emb_' in c]  # 768\n",
        "#     name_cols = [c for c in train.columns if 'name_emb_' in c]  # 768\n",
        "#     desc_cols = [c for c in train.columns if 'desc_emb_' in c]  # 768\n",
        "#     clip_cols = [c for c in train.columns if 'clip_' in c and 'emb' not in c]\n",
        "#     umap_cols = [c for c in train.columns if 'umap_' in c]  # 50\n",
        "#     cluster_cols = ['hdbscan_cluster', 'cluster_size', 'cluster_density',\n",
        "#                    'membership_strength', 'distance_to_center', 'is_noise',\n",
        "#                    'isolation_score', 'is_small_cluster', 'is_large_cluster']\n",
        "#     pca_cols = ['pc1', 'pc2']\n",
        "#     yolo_cols = [c for c in train.columns if 'yolo' in c or 'box_' in c or 'object_' in c]\n",
        "#     tabular_cols = ['value', 'total_value', 'unit', 'brand_cat', 'brand_tier',\n",
        "#                    'is_bulk', 'is_premium', 'has_size_descriptor']\n",
        "\n",
        "#     print(f\"üìä Feature breakdown:\")\n",
        "#     print(f\"   ViT embeddings: {len(vit_cols)}\")\n",
        "#     print(f\"   Name embeddings: {len(name_cols)}\")\n",
        "#     print(f\"   Desc embeddings: {len(desc_cols)}\")\n",
        "#     print(f\"   UMAP: {len(umap_cols)}\")\n",
        "#     print(f\"   Other features: ~{len(train.columns) - len(vit_cols) - len(name_cols) - len(desc_cols)}\")\n",
        "\n",
        "#     # Strategy: Reduce embeddings via SVD, keep engineered features\n",
        "#     def reduce_embeddings(train_data, test_data, cols, n_components):\n",
        "#         \"\"\"Apply TruncatedSVD to reduce embedding dimensions\"\"\"\n",
        "#         if len(cols) == 0:\n",
        "#             return train_data, test_data, []\n",
        "\n",
        "#         svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "\n",
        "#         train_reduced = svd.fit_transform(train_data[cols])\n",
        "#         test_reduced = svd.transform(test_data[cols])\n",
        "\n",
        "#         new_cols = [f\"{cols[0].split('_')[0]}_svd_{i}\" for i in range(n_components)]\n",
        "\n",
        "#         train_df = pd.DataFrame(train_reduced, columns=new_cols, index=train_data.index)\n",
        "#         test_df = pd.DataFrame(test_reduced, columns=new_cols, index=test_data.index)\n",
        "\n",
        "#         print(f\"   ‚úÖ Reduced {len(cols)} ‚Üí {n_components} ({svd.explained_variance_ratio_.sum():.3f} variance)\")\n",
        "\n",
        "#         return train_df, test_df, new_cols\n",
        "\n",
        "#     # Reduce each embedding group\n",
        "#     print(\"\\nüîß Reducing embeddings to prevent overfitting:\")\n",
        "\n",
        "#     vit_train, vit_test, vit_new = reduce_embeddings(train, test, vit_cols, 50)\n",
        "#     name_train, name_test, name_new = reduce_embeddings(train, test, name_cols, 30)\n",
        "#     desc_train, desc_test, desc_new = reduce_embeddings(train, test, desc_cols, 30)\n",
        "\n",
        "#     # Combine reduced embeddings + engineered features\n",
        "#     keep_cols = (umap_cols + cluster_cols + pca_cols + yolo_cols +\n",
        "#                  tabular_cols + clip_cols)\n",
        "\n",
        "#     # Filter to existing columns only\n",
        "#     keep_cols = [c for c in keep_cols if c in train.columns and c in test.columns]\n",
        "\n",
        "#     print(f\"\\n   Keeping {len(keep_cols)} engineered features\")\n",
        "\n",
        "#     # Build final datasets\n",
        "#     X_train = pd.concat([\n",
        "#         train[keep_cols],\n",
        "#         vit_train,\n",
        "#         name_train,\n",
        "#         desc_train\n",
        "#     ], axis=1)\n",
        "\n",
        "#     X_test = pd.concat([\n",
        "#         test[keep_cols],\n",
        "#         vit_test,\n",
        "#         name_test,\n",
        "#         desc_test\n",
        "#     ], axis=1)\n",
        "\n",
        "#     # Fill any remaining NaNs\n",
        "#     X_train = X_train.fillna(-999)\n",
        "#     X_test = X_test.fillna(-999)\n",
        "\n",
        "#     y_train = train['log_price']\n",
        "\n",
        "#     print(f\"\\n‚úÖ FINAL FEATURE COUNT: {X_train.shape[1]} (reduced from 2430)\")\n",
        "#     print(f\"   Feature-to-sample ratio: 1:{len(X_train) // X_train.shape[1]}\")\n",
        "\n",
        "#     return X_train, X_test, y_train\n",
        "\n",
        "# X_train, X_test, y_train = reduce_feature_dimensionality(train, test)\n"
      ],
      "metadata": {
        "id": "pO0ICK8eMgb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nüíæ Saving reduced features for reproducibility...\")\n",
        "\n",
        "# # Save to local disk (fast)\n",
        "# X_train.to_csv('/mnt/sagemaker-nvme/feature_merge/X_train_reduced.csv', index=False)\n",
        "# X_test.to_csv('/mnt/sagemaker-nvme/feature_merge/X_test_reduced.csv', index=False)\n",
        "# y_train.to_csv('/mnt/sagemaker-nvme/feature_merge/y_train.csv', index=False, header=['log_price'])\n",
        "\n",
        "# # Also save sample_id for final submission\n",
        "# train['sample_id'].to_csv('/mnt/sagemaker-nvme/feature_merge/train_sample_ids.csv', index=False, header=['sample_id'])\n",
        "# test['sample_id'].to_csv('/mnt/sagemaker-nvme/feature_merge/test_sample_ids.csv', index=False, header=['sample_id'])\n",
        "\n",
        "# print(f\"   ‚úÖ Saved X_train: {X_train.shape}\")\n",
        "# print(f\"   ‚úÖ Saved X_test: {X_test.shape}\")\n",
        "# print(f\"   ‚úÖ Saved y_train: {y_train.shape}\")\n",
        "\n",
        "# # Optional: Upload to S3 (if you have boto3 configured)\n",
        "# import boto3\n",
        "# s3 = boto3.client('s3')\n",
        "# bucket = 'amazon-ml-challenge-yourname'  # Change this!\n",
        "\n",
        "# try:\n",
        "#     s3.upload_file('/mnt/sagemaker-nvme/feature_merge/X_train_reduced.csv',\n",
        "#                    bucket, 'amazon_ml_challenge/X_train_reduced.csv')\n",
        "#     s3.upload_file('/mnt/sagemaker-nvme/feature_merge/X_test_reduced.csv',\n",
        "#                    bucket, 'amazon_ml_challenge/X_test_reduced.csv')\n",
        "#     print(f\"   ‚úÖ Uploaded to S3: {bucket}/amazon_ml_challenge/\")\n",
        "# except Exception as e:\n",
        "#     print(f\"   ‚ö†Ô∏è  S3 upload skipped: {e}\")"
      ],
      "metadata": {
        "id": "s3V-_5PYMgWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# LIGHTGBM TRAINING - FINAL (CPU)\n",
        "# ==============================\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" LIGHTGBM TRAINING - CPU MODE (NO MORE GPU ISSUES!) \")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def lgbm_smape_eval(y_true_log, y_pred_log):\n",
        "    \"\"\"Custom SMAPE for LightGBM\"\"\"\n",
        "    y_true = np.expm1(y_true_log)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    smape_score = np.mean(200 * numerator / (denominator + 1e-8))\n",
        "    return 'smape', smape_score, False\n",
        "\n",
        "def smape_metric(y_true, y_pred):\n",
        "    \"\"\"SMAPE for evaluation - matches lgbm_smape_eval formula\"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    return np.mean(100 * numerator / (denominator + 1e-8))\n",
        "\n",
        "\n",
        "def train_lgbm_final(X, y, X_test, n_folds=5):\n",
        "    \"\"\"LightGBM training - CPU optimized\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression_l1',  # MAE for SMAPE\n",
        "        'metric': 'mae',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.03,\n",
        "        'feature_fraction': 0.6,\n",
        "        'bagging_fraction': 0.7,\n",
        "        'bagging_freq': 5,\n",
        "        'min_child_samples': 50,\n",
        "        'min_split_gain': 0.1,\n",
        "        'reg_alpha': 1.0,\n",
        "        'reg_lambda': 3.0,\n",
        "        'n_estimators': 3000,\n",
        "        'n_jobs': -1,  # Use all CPU cores\n",
        "        'verbose': -1,\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(X))\n",
        "    test_preds = np.zeros(len(X_test))\n",
        "    smape_scores = []\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        fold_start = time.time()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üî• LightGBM Fold {fold + 1}/{n_folds}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=lgbm_smape_eval,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(150, verbose=False),\n",
        "                lgb.log_evaluation(500)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        oof_preds[val_idx] = model.predict(X_val)\n",
        "        test_preds += model.predict(X_test) / n_folds\n",
        "\n",
        "        # val_smape = smape_metric(np.expm1(y_val), np.expm1(oof_preds[val_idx]))\n",
        "        val_smape = smape_metric(np.expm1(y_val.values), np.expm1(oof_preds[val_idx]))\n",
        "\n",
        "        smape_scores.append(val_smape)\n",
        "\n",
        "        fold_time = (time.time() - fold_start) / 60\n",
        "        print(f\"\\n   üìä Fold {fold + 1} SMAPE: {val_smape:.4f}%\")\n",
        "        print(f\"   üìä Best iteration: {model.best_iteration_}\")\n",
        "        print(f\"   ‚è±Ô∏è  Fold time: {fold_time:.1f} mins\")\n",
        "\n",
        "        # Estimate remaining time\n",
        "        elapsed = (time.time() - start_time) / 60\n",
        "        avg_fold_time = elapsed / (fold + 1)\n",
        "        remaining = avg_fold_time * (n_folds - fold - 1)\n",
        "        print(f\"   ‚è±Ô∏è  Estimated remaining: {remaining:.1f} mins\")\n",
        "\n",
        "    avg_smape = np.mean(smape_scores)\n",
        "    std_smape = np.std(smape_scores)\n",
        "    total_time = (time.time() - start_time) / 60\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úÖ LightGBM CV SMAPE: {avg_smape:.4f}% ¬± {std_smape:.4f}%\")\n",
        "    print(f\"‚è±Ô∏è  Total training time: {total_time:.1f} mins\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return oof_preds, test_preds, avg_smape, std_smape\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0MXQqAzMgT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# === FIX COLUMN NAMES (ADD THIS NOW!) ===\n",
        "print(\"\\nüîß Cleaning column names for LightGBM...\")\n",
        "\n",
        "def clean_column_names(df):\n",
        "    \"\"\"Remove special JSON characters from column names\"\"\"\n",
        "    # Remove all special characters except letters, numbers, and underscores\n",
        "    new_names = {col: re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns}\n",
        "\n",
        "    # Handle duplicates (add suffix if name already exists)\n",
        "    new_n_list = list(new_names.values())\n",
        "    final_names = {}\n",
        "    for i, (old_col, new_col) in enumerate(new_names.items()):\n",
        "        if new_col in new_n_list[:i]:\n",
        "            final_names[old_col] = f'{new_col}_{i}'\n",
        "        else:\n",
        "            final_names[old_col] = new_col\n",
        "\n",
        "    return df.rename(columns=final_names)\n",
        "\n",
        "# Clean both train and test\n",
        "X_train = clean_column_names(X_train)\n",
        "X_test = clean_column_names(X_test)\n",
        "\n",
        "print(f\"‚úÖ Cleaned {len(X_train.columns)} column names\")\n",
        "print(f\"   Sample columns: {list(X_train.columns[:5])}\")\n",
        "\n",
        "# Now training will work!\n"
      ],
      "metadata": {
        "id": "q0uBVExoMgRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# START TRAINING NOW!\n",
        "lgb_oof, lgb_test, lgb_score, lgb_std = train_lgbm_final(\n",
        "    X_train, y_train, X_test, n_folds=5\n",
        ")"
      ],
      "metadata": {
        "id": "5jLrUOPkMgOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tthQI2pMgMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# CATBOOST GPU - USING BUILT-IN SMAPE\n",
        "# ==============================\n",
        "\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" CATBOOST GPU TRAINING - BUILT-IN SMAPE METRIC \")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def smape_metric_display(y_true, y_pred):\n",
        "    \"\"\"SMAPE for display (0-200 scale to match LightGBM)\"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "    return np.mean(100 * numerator / (denominator + 1e-8))\n",
        "\n",
        "def train_catboost_gpu(X, y, X_test, n_folds=5):\n",
        "    \"\"\"CatBoost GPU - using built-in SMAPE\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'task_type': 'GPU',\n",
        "        'devices': '0',\n",
        "        'loss_function': 'MAE',\n",
        "        'eval_metric': 'SMAPE',\n",
        "        'iterations': 4000,\n",
        "        'learning_rate': 0.03,\n",
        "        'depth': 6,\n",
        "        'l2_leaf_reg': 5,\n",
        "        'random_strength': 0.5,\n",
        "        'bagging_temperature': 0.2,\n",
        "        'min_data_in_leaf': 50,\n",
        "        'bootstrap_type': 'Bayesian',\n",
        "        'early_stopping_rounds': 200,\n",
        "        'use_best_model': True,\n",
        "        'random_seed': 42,\n",
        "        'verbose': 200,\n",
        "        'allow_writing_files': False\n",
        "    }\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(X))\n",
        "    test_preds = np.zeros(len(X_test))\n",
        "    smape_scores = []\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        fold_start = time.time()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ CatBoost Fold {fold + 1}/{n_folds}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        train_data = cb.Pool(X_tr, y_tr)\n",
        "        val_data = cb.Pool(X_val, y_val)\n",
        "\n",
        "        model = cb.CatBoostRegressor(**params)\n",
        "        model.fit(\n",
        "            train_data,\n",
        "            eval_set=val_data,\n",
        "            use_best_model=True,\n",
        "            verbose=False,\n",
        "            plot=False\n",
        "        )\n",
        "\n",
        "        oof_preds[val_idx] = model.predict(X_val)\n",
        "        test_preds += model.predict(X_test) / n_folds\n",
        "\n",
        "        # Calculate SMAPE on original scale (0-200 scale)\n",
        "        val_smape = smape_metric_display(np.expm1(y_val.values), np.expm1(oof_preds[val_idx]))\n",
        "        smape_scores.append(val_smape)\n",
        "\n",
        "        fold_time = (time.time() - fold_start) / 60\n",
        "        print(f\"\\n   üìä Fold {fold + 1} SMAPE: {val_smape:.4f}%\")\n",
        "        print(f\"   üìä Best iteration: {model.get_best_iteration()}\")\n",
        "        print(f\"   ‚è±Ô∏è  Fold time: {fold_time:.1f} mins\")\n",
        "\n",
        "        elapsed = (time.time() - start_time) / 60\n",
        "        avg_fold_time = elapsed / (fold + 1)\n",
        "        remaining = avg_fold_time * (n_folds - fold - 1)\n",
        "        print(f\"   ‚è±Ô∏è  Estimated remaining: {remaining:.1f} mins\")\n",
        "\n",
        "    avg_smape = np.mean(smape_scores)\n",
        "    std_smape = np.std(smape_scores)\n",
        "    total_time = (time.time() - start_time) / 60\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úÖ CatBoost CV SMAPE: {avg_smape:.4f}% ¬± {std_smape:.4f}%\")\n",
        "    print(f\"‚è±Ô∏è  Total training time: {total_time:.1f} mins\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return oof_preds, test_preds, avg_smape, std_smape\n",
        "\n"
      ],
      "metadata": {
        "id": "6qFik6VHMgHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# START TRAINING\n",
        "cat_oof, cat_test, cat_score, cat_std = train_catboost_gpu(\n",
        "    X_train, y_train, X_test, n_folds=5\n",
        ")\n",
        "\n",
        "# Save predictions\n",
        "np.save('/mnt/sagemaker-nvme/feature_merge/cat_oof.npy', cat_oof)\n",
        "np.save('/mnt/sagemaker-nvme/feature_merge/cat_test.npy', cat_test)\n",
        "\n",
        "print(f\"\\nüíæ Saved CatBoost predictions\")\n",
        "print(f\"‚úÖ CatBoost training complete!\")"
      ],
      "metadata": {
        "id": "VpzvbS3zNtTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# SIMPLE ENSEMBLE - NO OPTIMIZATION NEEDED\n",
        "# ==============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" CREATING ENSEMBLE - SIMPLE AVERAGE \")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load predictions\n",
        "lgb_oof = np.load('/mnt/sagemaker-nvme/feature_merge/lgb_oof.npy')\n",
        "lgb_test = np.load('/mnt/sagemaker-nvme/feature_merge/lgb_test.npy')\n",
        "cat_oof = np.load('/mnt/sagemaker-nvme/feature_merge/cat_oof.npy')\n",
        "cat_test = np.load('/mnt/sagemaker-nvme/feature_merge/cat_test.npy')\n",
        "\n",
        "print(f\"‚úÖ Loaded all predictions\")\n",
        "\n",
        "# SMAPE metric\n",
        "# def smape_metric(y_true, y_pred):\n",
        "#     \"\"\"SMAPE (0-100 scale)\"\"\"\n",
        "#     numerator = np.abs(y_pred - y_true)\n",
        "#     denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "#     return np.mean(numerator / (denominator + 1e-8)) * 100\n",
        "\n",
        "# Calculate individual model scores\n",
        "y_true = y_train.values\n",
        "# lgb_smape = smape_metric(np.expm1(y_true), np.expm1(lgb_oof))\n",
        "# cat_smape = smape_metric(np.expm1(y_true), np.expm1(cat_oof))\n",
        "\n",
        "# print(f\"\\nüìä Individual Model Scores:\")\n",
        "# print(f\"   LightGBM: {lgb_smape:.4f}%\")\n",
        "# print(f\"   CatBoost: {cat_smape:.4f}%\")\n",
        "\n",
        "# Simple average ensemble (best for similar-performing models)\n",
        "ensemble_oof = (lgb_oof + cat_oof) / 2\n",
        "ensemble_test = (lgb_test + cat_test) / 2\n",
        "\n",
        "# Calculate ensemble score\n",
        "# ensemble_smape = smape_metric(np.expm1(y_true), np.expm1(ensemble_oof))\n",
        "\n",
        "# print(f\"\\nüèÜ Ensemble Score:\")\n",
        "# print(f\"   Simple Average: {ensemble_smape:.4f}%\")\n",
        "# print(f\"   Improvement: {min(lgb_smape, cat_smape) - ensemble_smape:.4f}%\")\n",
        "\n",
        "# Convert to original scale\n",
        "final_prices = np.expm1(ensemble_test)\n",
        "\n",
        "# Clip to reasonable range\n",
        "min_price = 0.5\n",
        "max_price = np.expm1(y_true).max() * 1.2\n",
        "final_prices = np.clip(final_prices, min_price, max_price)\n",
        "\n",
        "print(f\"\\n‚úÖ Final predictions:\")\n",
        "print(f\"   Price range: [{final_prices.min():.2f}, {final_prices.max():.2f}]\")\n",
        "print(f\"   Mean: {final_prices.mean():.2f}\")\n",
        "print(f\"   Median: {np.median(final_prices):.2f}\")\n"
      ],
      "metadata": {
        "id": "bYJHbKhnNta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# CREATE SUBMISSION\n",
        "# ==============================\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'sample_id': test['sample_id'].values,\n",
        "    'price': final_prices\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('submission_final.csv', index=False)\n",
        "\n",
        "print(f\"\\nüíæ SUBMISSION SAVED!\")\n",
        "print(f\"   File: submission_final.csv\")\n",
        "print(f\"   Rows: {len(submission)}\")\n",
        "\n",
        "# Validation checks\n",
        "print(f\"\\nüîç Submission Validation:\")\n",
        "print(f\"   No missing values: {submission['price'].isna().sum() == 0}\")\n",
        "print(f\"   No negative prices: {(submission['price'] < 0).sum() == 0}\")\n",
        "print(f\"   Correct shape: {submission.shape == (75000, 2)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" üéâ READY TO SUBMIT! \")\n",
        "print(\"=\"*80)\n",
        "print(f\"Expected Leaderboard Score: ~{ensemble_smape:.2f}%\")\n",
        "print(f\"Improvement vs Baseline (54.38%): {54.38 - ensemble_smape:.2f}%\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "vXPxJc9ANtdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvGloOVcNtgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRC7Ku54Ntiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iqc_UqX5NtlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdgfpM3gNtoS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}