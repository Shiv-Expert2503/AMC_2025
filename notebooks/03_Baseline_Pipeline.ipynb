{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc693c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration Block ---\n",
    "class CFG:\n",
    "    TRAIN_FILE = 'train.csv'\n",
    "    TEST_FILE = 'train.csv'\n",
    "    EMBEDDING_MODEL = 'intfloat/e5-base-v2'\n",
    "    RANDOM_SEED = 42\n",
    "    TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfbd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most robust IPQ function we've built\n",
    "def find_ipq(text):\n",
    "    text = str(text)\n",
    "    pattern_before = r'(?:pack of|case of|set of|bundle of|pk)\\s*(\\d+)'\n",
    "    match = re.search(pattern_before, text, re.IGNORECASE)\n",
    "    if match: return int(match.group(1))\n",
    "    pattern_per = r'(\\d+)\\s*per\\s*case'\n",
    "    match = re.search(pattern_per, text, re.IGNORECASE)\n",
    "    if match: return int(match.group(1))\n",
    "    pattern_after = r'(\\d+)\\s*[-]?\\s*(?:pack|count|pk|ct|pcs|case|pouch|pouches|servings|bottles|cans|bars|rolls|units)'\n",
    "    match = re.search(pattern_after, text, re.IGNORECASE)\n",
    "    if match: return int(match.group(1))\n",
    "    pattern_x = r'\\(?x\\s*(\\d+)\\)?'\n",
    "    match = re.search(pattern_x, text, re.IGNORECASE)\n",
    "    if match: return int(match.group(1))\n",
    "    return 1\n",
    "\n",
    "def parse_full_content_final(text):\n",
    "    if pd.isna(text):\n",
    "        text = \"\"\n",
    "    else:\n",
    "        text = str(text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    item_name_match = re.search(r'Item Name:\\s*(.*?)(?=\\s*Bullet Point|Product Description|Value|$)', text, re.DOTALL)\n",
    "    item_name = item_name_match.group(1).strip().lower() if item_name_match else None\n",
    "    bullets = re.findall(r'Bullet Point(?:\\s+\\d+)?:(.*?)(?=\\s*Bullet Point|Product Description|Value|$)', text, re.DOTALL)\n",
    "    bullet_points_text = ' '.join([b.strip() for b in bullets]).strip()\n",
    "    desc_match = re.search(r'Product Description:\\s*(.*?)(?=\\s*Value|$)', text, re.DOTALL)\n",
    "    product_description_text = desc_match.group(1).strip() if desc_match else ''\n",
    "    description_parts = [part for part in [bullet_points_text, product_description_text] if part]\n",
    "    description = ' '.join(description_parts).lower() if description_parts else None\n",
    "    ipq = find_ipq(text)\n",
    "    value_match = re.search(r'.*Value:\\s*([\\d\\.]+)', text, re.DOTALL)\n",
    "    unit_match = re.search(r'.*Unit:\\s*(.*)', text, re.DOTALL)\n",
    "    value = float(value_match.group(1).strip()) if value_match else np.nan\n",
    "    unit = unit_match.group(1).strip().lower() if unit_match else None\n",
    "    return {'item_name': item_name, 'description': description, 'ipq': ipq, 'value': value, 'unit': unit}\n",
    "\n",
    "UNIT_CONVERSION_MAP = {\n",
    "    'fluid ounce(s)': ('fl oz', 1.0), 'fluid ounce': ('fl oz', 1.0), 'fl. oz.': ('fl oz', 1.0),\n",
    "    'fl. oz': ('fl oz', 1.0), 'fl oz': ('fl oz', 1.0), 'floz': ('fl oz', 1.0),\n",
    "    'liter': ('fl oz', 33.814), 'liters': ('fl oz', 33.814), 'ltr': ('fl oz', 33.814),\n",
    "    'milliliter': ('fl oz', 0.033814), 'millilitre': ('fl oz', 0.033814),\n",
    "    'mililitro': ('fl oz', 0.033814), 'ml': ('fl oz', 0.033814), 'ounce': ('oz', 1.0),\n",
    "    'ounces': ('oz', 1.0), 'oz': ('oz', 1.0), 'pound': ('oz', 16.0), 'pounds': ('oz', 16.0),\n",
    "    'lb': ('oz', 16.0), 'gram': ('oz', 0.035274), 'grams(gm)': ('oz', 0.035274),\n",
    "    'grams': ('oz', 0.035274), 'gramm': ('oz', 0.035274), 'gr': ('oz', 0.035274),\n",
    "    'kg': ('oz', 35.274), 'count': ('ct', 1.0), 'ct': ('ct', 1.0), 'each': ('ct', 1.0),\n",
    "    'piece': ('ct', 1.0), 'packs': ('ct', 1.0), 'pack': ('ct', 1.0),\n",
    "    'bottle': ('ct', 1.0), 'bottles': ('ct', 1.0), 'bag': ('ct', 1.0),\n",
    "    'bags': ('ct', 1.0), 'can': ('ct', 1.0), 'jar': ('ct', 1.0), 'k-cups': ('ct', 1.0),\n",
    "    'per carton': ('ct', 1.0), 'pouch': ('ct', 1.0), 'per package': ('ct', 1.0),\n",
    "    'per box': ('ct', 1.0), 'paper cupcake liners': ('ct', 1.0), 'capsule': ('ct', 1.0),\n",
    "    'carton': ('ct', 1.0), 'ziplock bags': ('ct', 1.0), 'units': ('ct', 1.0),\n",
    "    'box': ('ct', 1.0), 'bucket': ('ct', 1.0), 'none': ('None', 1.0), '---': ('None', 1.0)\n",
    "}\n",
    "\n",
    "unit_keys = '|'.join(re.escape(k) for k in UNIT_CONVERSION_MAP.keys())\n",
    "VALUE_IN_TITLE_REGEX = re.compile(r'(\\d+(?:\\.\\d*)?|\\.\\d+)\\s*(' + unit_keys + r')\\b|(' + unit_keys + r')\\s*(\\d+(?:\\.\\d*)?|\\.\\d+)\\b', re.IGNORECASE)\n",
    "\n",
    "def extract_and_standardize_from_title(title):\n",
    "    if pd.isna(title): return np.nan, None\n",
    "    match = VALUE_IN_TITLE_REGEX.search(str(title))\n",
    "    if not match: return np.nan, None\n",
    "    if match.group(1): value_str, unit_str = match.group(1), match.group(2)\n",
    "    else: unit_str, value_str = match.group(3), match.group(4)\n",
    "    value = float(value_str)\n",
    "    unit_lower = unit_str.lower()\n",
    "    standard_unit, conversion_factor = UNIT_CONVERSION_MAP.get(unit_lower, (None, 1.0))\n",
    "    if standard_unit: return value * conversion_factor, standard_unit\n",
    "    return np.nan, None\n",
    "\n",
    "def standardize_original_value(value, unit):\n",
    "    if pd.isna(value) or pd.isna(unit): return np.nan, None\n",
    "    unit_lower = str(unit).lower()\n",
    "    if unit_lower in UNIT_CONVERSION_MAP:\n",
    "        standard_unit, conversion_factor = UNIT_CONVERSION_MAP[unit_lower]\n",
    "        return value * conversion_factor, standard_unit\n",
    "    for keyword, (mapping, conversion) in UNIT_CONVERSION_MAP.items():\n",
    "        if keyword in unit_lower: return value * conversion, mapping\n",
    "    return value, 'other'\n",
    "\n",
    "def rectify_value_and_unit(row):\n",
    "    value_from_title, unit_from_title = extract_and_standardize_from_title(row['item_name'])\n",
    "    value_from_content, unit_from_content = standardize_original_value(row['value'], row['unit'])\n",
    "    ipq = row['ipq'] if row['ipq'] > 0 else 1\n",
    "    if pd.notna(value_from_title):\n",
    "        final_value = value_from_title\n",
    "        final_unit = unit_from_title\n",
    "    else:\n",
    "        final_value = value_from_content / ipq if pd.notna(value_from_content) else np.nan\n",
    "        final_unit = unit_from_content\n",
    "    return pd.Series([final_value, final_unit])\n",
    "\n",
    "def extract_brand_by_length(text):\n",
    "    if not isinstance(text, str) or not text: return \"Unknown\"\n",
    "    words = text.split()\n",
    "    if len(words) == 0: return \"Unknown\"\n",
    "    if len(words) == 1: return words[0]\n",
    "    first_word = words[0]\n",
    "    if len(first_word) > 2: return first_word\n",
    "    else: return ' '.join(words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa8b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_parse_data(file_path):\n",
    "    \"\"\"Loads data and applies the initial parsing function.\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors='coerce')\n",
    "    \n",
    "    print(\"Parsing catalog_content...\")\n",
    "    extracted_features = df['catalog_content'].progress_apply(parse_full_content_final).apply(pd.Series)\n",
    "    df = pd.concat([df, extracted_features], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_base_features(df):\n",
    "    \"\"\"Creates brand, value, and unit features.\"\"\"\n",
    "    print(\"Creating base features (brand, value, unit)...\")\n",
    "    df[['value_final', 'unit_final']] = df.progress_apply(rectify_value_and_unit, axis=1)\n",
    "    \n",
    "    # Apply manual rules\n",
    "    df['value_final'] = np.where((df['unit_final'] == 'ct') & (df['ipq'] == df['value']), 1.0, df['value_final'])\n",
    "    df['value_final'] = np.where(df['value_final'] > 1000, df['value'], df['value_final'])\n",
    "    df['ipq'] = np.where(df['ipq'] > 5000, 5000, df['ipq'])\n",
    "    \n",
    "    df[\"value\"] = df[\"value_final\"].combine_first(df[\"value\"])\n",
    "    df[\"unit\"] = df[\"unit_final\"]\n",
    "    df = df.drop(columns=[\"value_final\", \"unit_final\"])\n",
    "    \n",
    "    df['total_value'] = df['ipq'] * df['value']\n",
    "    df['brand'] = df['item_name'].progress_apply(extract_brand_by_length)\n",
    "    \n",
    "    # Target variable\n",
    "    df['log_price'] = np.log1p(df['price'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_text_embeddings(df, text_column, prefix):\n",
    "    \"\"\"Generates and adds text embeddings for a specified column.\"\"\"\n",
    "    print(f\"Generating embeddings for '{text_column}'...\")\n",
    "    \n",
    "    if torch.cuda.is_available(): device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "    \n",
    "    model = SentenceTransformer(CFG.EMBEDDING_MODEL, device=device)\n",
    "    \n",
    "    texts = df[text_column].fillna('').tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=128)\n",
    "    \n",
    "    emb_df = pd.DataFrame(embeddings, columns=[f'{prefix}_{i}' for i in range(embeddings.shape[1])])\n",
    "    \n",
    "    return pd.concat([df.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "def finalize_features(df):\n",
    "    \"\"\"Prepares the final feature set for modeling.\"\"\"\n",
    "    print(\"Finalizing features for modeling...\")\n",
    "    \n",
    "    # Create categorical codes\n",
    "    df['brand_cat'] = pd.factorize(df['brand'])[0]\n",
    "    df['unit_cat'] = pd.factorize(df['unit'])[0]\n",
    "    \n",
    "    # Drop original text columns\n",
    "    df = df.drop(columns=['catalog_content', 'item_name', 'description', 'brand', 'unit', 'image_link', 'price'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb2f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculates SMAPE, robust to zero values.\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "def train_and_evaluate_lgbm(df):\n",
    "    \"\"\"Trains a LightGBM model and evaluates it using SMAPE.\"\"\"\n",
    "    print(\"\\n--- Training LightGBM Model ---\")\n",
    "    \n",
    "    y = df['log_price']\n",
    "    X = df.drop(columns=['log_price', 'sample_id'])\n",
    "    \n",
    "    categorical_features = ['brand_cat', 'unit_cat']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=CFG.TEST_SIZE, random_state=CFG.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    lgbm = lgb.LGBMRegressor(random_state=CFG.RANDOM_SEED, device='cpu') # Use 'gpu' for NVIDIA\n",
    "    \n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[lgb.early_stopping(15, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_log = lgbm.predict(X_val)\n",
    "    y_pred_actual = np.expm1(y_pred_log)\n",
    "    y_true_actual = np.expm1(y_val)\n",
    "    \n",
    "    smape_score = smape(y_true_actual, y_pred_actual)\n",
    "    print(f\"✅ Validation SMAPE Score: {smape_score:.4f}%\")\n",
    "    \n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f548cd1-1f25-4604-b975-ea5db032e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def train_and_evaluate_ridge(df, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Trains a Ridge Regression model and evaluates it using SMAPE.\n",
    "    Includes a step to handle missing NaN values.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training Ridge Regression Model ---\")\n",
    "\n",
    "    y = df['log_price']\n",
    "    X = df.drop(columns=['log_price', 'sample_id'], errors='ignore')\n",
    "\n",
    "    # --- FIX: Handle missing values before training ---\n",
    "    # We will fill any NaN in the feature set with the median of its column.\n",
    "    # This is a robust way to handle missing data for models like Ridge.\n",
    "    print(\"Handling missing values by filling with column median...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y,\n",
    "        test_size=CFG.TEST_SIZE,\n",
    "        random_state=CFG.RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Initialize the Ridge model\n",
    "    ridge = Ridge(alpha=alpha, random_state=CFG.RANDOM_SEED)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Fitting Ridge model...\")\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_log = ridge.predict(X_val)\n",
    "    y_pred_actual = np.expm1(y_pred_log)\n",
    "    y_true_actual = np.expm1(y_val)\n",
    "\n",
    "    # Calculate and print the SMAPE score\n",
    "    smape_score = smape(y_true_actual, y_pred_actual)\n",
    "    print(f\"✅ Validation SMAPE Score: {smape_score:.4f}%\")\n",
    "\n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "658b684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from train.csv...\n",
      "Parsing catalog_content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa928d7e9b1e43faa59a6fa77bc2e0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base features (brand, value, unit)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9bb50b2e18416b9fbc5d2b8240c6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d2080b76ef44f3988b63719056d7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 'item_name'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82ce5c99d794ebc9d41a9f4e2075a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 'description'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d6d49d65074c0a9019c5317b4f5deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing features for modeling...\n",
      "\n",
      "--- Training LightGBM Model ---\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395780\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 1541\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "✅ Validation SMAPE Score: 54.3818%\n"
     ]
    }
   ],
   "source": [
    "# --- Run the Full Pipeline ---\n",
    "tqdm.pandas()\n",
    "\n",
    "# 1. Load and Parse\n",
    "train_df = load_and_parse_data(CFG.TRAIN_FILE)\n",
    "\n",
    "# 2. Create Base Features\n",
    "train_df = create_base_features(train_df)\n",
    "\n",
    "# 3. Generate Embeddings (Modular)\n",
    "train_df = generate_text_embeddings(train_df, text_column='item_name', prefix='name_emb')\n",
    "train_df = generate_text_embeddings(train_df, text_column='description', prefix='desc_emb')\n",
    "\n",
    "# 4. Finalize Features\n",
    "train_final = finalize_features(train_df)\n",
    "\n",
    "# 5. Train and Evaluate Model\n",
    "model = train_and_evaluate_lgbm(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0bf7c35-3d99-4ecd-8bfc-71e5a3e18feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from train.csv...\n",
      "Parsing catalog_content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00b9de29dce41ff949450b70e448b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base features (brand, value, unit)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5a95a982674b18b51db3e4a635bf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757702256434487a8218f9616cd1381b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 'item_name'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecac59bd880d41feba7d887a79a26db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing features for modeling...\n",
      "\n",
      "--- Training LightGBM Model ---\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 199940\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 773\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "✅ Validation SMAPE Score: 54.3609%\n"
     ]
    }
   ],
   "source": [
    "# --- Run the Full Pipeline ---\n",
    "tqdm.pandas()\n",
    "\n",
    "# 1. Load and Parse\n",
    "train_df = load_and_parse_data(CFG.TRAIN_FILE)\n",
    "\n",
    "# 2. Create Base Features\n",
    "train_df = create_base_features(train_df)\n",
    "\n",
    "# 3. Generate Embeddings (Modular)\n",
    "train_df = generate_text_embeddings(train_df, text_column='item_name', prefix='name_emb')\n",
    "\n",
    "# 4. Finalize Features\n",
    "train_final = finalize_features(train_df)\n",
    "\n",
    "# 5. Train and Evaluate Model\n",
    "model = train_and_evaluate_lgbm(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be942c0-3bfc-4293-bc49-edd70d16bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Ridge Regression Model ---\n",
      "Handling missing values by filling with column median...\n",
      "Fitting Ridge model...\n",
      "✅ Validation SMAPE Score: 59.5602%\n"
     ]
    }
   ],
   "source": [
    "# Run the Ridge regression training and evaluation\n",
    "ridge_model = train_and_evaluate_ridge(train_final, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "540c48f6-aa37-4678-9c78-029d42759d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM Model ---\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395780\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 1541\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "\n",
      "--- Model Evaluation Complete ---\n",
      "✅ Overall Validation SMAPE Score: 54.3818%\n",
      "\n",
      "--- Validation Set with Predicted Prices and SMAPE Scores ---\n",
      "       sample_id                                          item_name   price  \\\n",
      "26837     158784  log cabin sugar free syrup, 24 fl oz (pack of 12)  12.195   \n",
      "2592        4095  raspberry ginseng oolong tea (50 tea bags, zin...  38.540   \n",
      "18359     172021  walden farms honey dijon dressing - calorie-fr...  17.860   \n",
      "73292     268276  vlasic ovals hamburger dill pickle chips, keto...   2.940   \n",
      "60127     154791  amoretti premium syrup, grand orange, 25.4 oun...  25.990   \n",
      "\n",
      "       predicted_price  smape_score  \n",
      "26837        15.971958    26.818362  \n",
      "2592         45.818878    17.256933  \n",
      "18359         7.787202    78.548905  \n",
      "73292         3.937870    29.016820  \n",
      "60127        28.596941     9.551519  \n",
      "\n",
      "--- Top 5 Worst Performing Rows in Validation Set ---\n",
      "                                               item_name    price  \\\n",
      "74116  kool aid watermelon drink mix, makes 2 quarts ...     0.37   \n",
      "58617  4patriots 1-year survival food kit: emergency ...  2796.00   \n",
      "28165  donettes single serve crunch mini donuts, 4 ou...     1.68   \n",
      "18709           nescaf clasico decaf instant coffee 7 oz   286.77   \n",
      "38615  land o lakes arctic white hot cocoa classics m...     0.53   \n",
      "\n",
      "       predicted_price  smape_score  \n",
      "74116        24.266732   193.992710  \n",
      "58617        46.741277   193.423070  \n",
      "28165        87.027934   192.424578  \n",
      "18709         6.626453   190.965872  \n",
      "38615        19.897279   189.621721  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Modified Training Function to Return Predictions ---\n",
    "def train_and_get_predictions_lgbm(df):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model and returns the model, validation predictions,\n",
    "    and true validation labels.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training LightGBM Model ---\")\n",
    "    \n",
    "    y = df['log_price']\n",
    "    X = df.drop(columns=['log_price', 'sample_id'], errors='ignore')\n",
    "    \n",
    "    categorical_features = ['brand_cat', 'unit_cat']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=CFG.TEST_SIZE, random_state=CFG.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    lgbm = lgb.LGBMRegressor(random_state=CFG.RANDOM_SEED, device='cpu')\n",
    "    \n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[lgb.early_stopping(15, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_log = lgbm.predict(X_val)\n",
    "    \n",
    "    return lgbm, y_pred_log, y_val\n",
    "\n",
    "# --- 2. Run Training and Get Predictions ---\n",
    "model, y_pred_log, y_val = train_and_get_predictions_lgbm(train_final)\n",
    "\n",
    "# --- 3. Calculate and Print Overall SMAPE ---\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "y_true_actual = np.expm1(y_val)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "overall_smape_score = smape(y_true_actual, y_pred_actual)\n",
    "print(\"\\n--- Model Evaluation Complete ---\")\n",
    "print(f\"✅ Overall Validation SMAPE Score: {overall_smape_score:.4f}%\")\n",
    "\n",
    "\n",
    "# --- 4. Perform Detailed Validation Set Analysis ---\n",
    "train_set, val_set = train_test_split(\n",
    "    train_df,\n",
    "    test_size=CFG.TEST_SIZE,\n",
    "    random_state=CFG.RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_results_df = val_set.copy()\n",
    "val_results_df['predicted_price'] = y_pred_actual\n",
    "\n",
    "def calculate_smape_per_row(row):\n",
    "    true = row['price']\n",
    "    pred = row['predicted_price']\n",
    "    numerator = np.abs(pred - true)\n",
    "    denominator = (np.abs(true) + np.abs(pred)) / 2\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    return (numerator / denominator) * 100\n",
    "\n",
    "val_results_df['smape_score'] = val_results_df.apply(calculate_smape_per_row, axis=1)\n",
    "\n",
    "# --- 5. Display Detailed Results ---\n",
    "print(\"\\n--- Validation Set with Predicted Prices and SMAPE Scores ---\")\n",
    "print(val_results_df[[\n",
    "    'sample_id', 'item_name', 'price', 'predicted_price', 'smape_score'\n",
    "]].head())\n",
    "\n",
    "print(\"\\n--- Top 5 Worst Performing Rows in Validation Set ---\")\n",
    "print(val_results_df.sort_values(by='smape_score', ascending=False).head(5)[[\n",
    "    'item_name', 'price', 'predicted_price', 'smape_score'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea82cad-bace-45d5-a869-40ea1046bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM Model ---\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395780\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 1541\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "\n",
      "--- Model Evaluation Complete ---\n",
      "✅ Overall Validation SMAPE Score: 54.3818%\n",
      "\n",
      "--- Validation Set with Predicted Prices and SMAPE Scores ---\n",
      "       sample_id                                          item_name   price  \\\n",
      "26837     158784  log cabin sugar free syrup, 24 fl oz (pack of 12)  12.195   \n",
      "2592        4095  raspberry ginseng oolong tea (50 tea bags, zin...  38.540   \n",
      "18359     172021  walden farms honey dijon dressing - calorie-fr...  17.860   \n",
      "73292     268276  vlasic ovals hamburger dill pickle chips, keto...   2.940   \n",
      "60127     154791  amoretti premium syrup, grand orange, 25.4 oun...  25.990   \n",
      "\n",
      "       predicted_price  smape_score  \n",
      "26837        15.971958    26.818362  \n",
      "2592         45.818878    17.256933  \n",
      "18359         7.787202    78.548905  \n",
      "73292         3.937870    29.016820  \n",
      "60127        28.596941     9.551519  \n",
      "\n",
      "--- Top 5 Worst Performing Rows in Validation Set ---\n",
      "                                               item_name    price  \\\n",
      "74116  kool aid watermelon drink mix, makes 2 quarts ...     0.37   \n",
      "58617  4patriots 1-year survival food kit: emergency ...  2796.00   \n",
      "28165  donettes single serve crunch mini donuts, 4 ou...     1.68   \n",
      "18709           nescaf clasico decaf instant coffee 7 oz   286.77   \n",
      "38615  land o lakes arctic white hot cocoa classics m...     0.53   \n",
      "\n",
      "       predicted_price  smape_score  \n",
      "74116        24.266732   193.992710  \n",
      "58617        46.741277   193.423070  \n",
      "28165        87.027934   192.424578  \n",
      "18709         6.626453   190.965872  \n",
      "38615        19.897279   189.621721  \n",
      "\n",
      "--- Saving Top 1000 Worst Performing Samples ---\n",
      "✅ Successfully saved the top 1000 worst predictions to 'worst_performing_samples.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Modified Training Function to Return Predictions ---\n",
    "def train_and_get_predictions_lgbm(df):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model and returns the model, validation predictions,\n",
    "    and true validation labels.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training LightGBM Model ---\")\n",
    "    \n",
    "    y = df['log_price']\n",
    "    X = df.drop(columns=['log_price', 'sample_id'], errors='ignore')\n",
    "    \n",
    "    categorical_features = ['brand_cat', 'unit_cat']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=CFG.TEST_SIZE, random_state=CFG.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    lgbm = lgb.LGBMRegressor(random_state=CFG.RANDOM_SEED, device='cpu')\n",
    "    \n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[lgb.early_stopping(15, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_log = lgbm.predict(X_val)\n",
    "    \n",
    "    return lgbm, y_pred_log, y_val\n",
    "\n",
    "# --- 2. Run Training and Get Predictions ---\n",
    "model, y_pred_log, y_val = train_and_get_predictions_lgbm(train_final)\n",
    "\n",
    "# --- 3. Calculate and Print Overall SMAPE ---\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "y_true_actual = np.expm1(y_val)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "overall_smape_score = smape(y_true_actual, y_pred_actual)\n",
    "print(\"\\n--- Model Evaluation Complete ---\")\n",
    "print(f\"✅ Overall Validation SMAPE Score: {overall_smape_score:.4f}%\")\n",
    "\n",
    "\n",
    "# --- 4. Perform Detailed Validation Set Analysis ---\n",
    "train_set, val_set = train_test_split(\n",
    "    train_df,\n",
    "    test_size=CFG.TEST_SIZE,\n",
    "    random_state=CFG.RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_results_df = val_set.copy()\n",
    "val_results_df['predicted_price'] = y_pred_actual\n",
    "\n",
    "def calculate_smape_per_row(row):\n",
    "    true = row['price']\n",
    "    pred = row['predicted_price']\n",
    "    numerator = np.abs(pred - true)\n",
    "    denominator = (np.abs(true) + np.abs(pred)) / 2\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    return (numerator / denominator) * 100\n",
    "\n",
    "val_results_df['smape_score'] = val_results_df.apply(calculate_smape_per_row, axis=1)\n",
    "\n",
    "# --- 5. Display Detailed Results ---\n",
    "print(\"\\n--- Validation Set with Predicted Prices and SMAPE Scores ---\")\n",
    "print(val_results_df[[\n",
    "    'sample_id', 'item_name', 'price', 'predicted_price', 'smape_score'\n",
    "]].head())\n",
    "\n",
    "print(\"\\n--- Top 5 Worst Performing Rows in Validation Set ---\")\n",
    "print(val_results_df.sort_values(by='smape_score', ascending=False).head(5)[[\n",
    "    'item_name', 'price', 'predicted_price', 'smape_score'\n",
    "]])\n",
    "\n",
    "# --- NEW: Step 6. Save the Top 1000 Worst Performing Rows ---\n",
    "print(\"\\n--- Saving Top 1000 Worst Performing Samples ---\")\n",
    "\n",
    "# Sort the entire validation results DataFrame by SMAPE score in descending order\n",
    "worst_samples_df = val_results_df.sort_values(by='smape_score', ascending=False)\n",
    "\n",
    "# Select the top 1000 rows\n",
    "top_1000_worst = worst_samples_df.head(1000)\n",
    "\n",
    "# Define the columns you want to save for analysis\n",
    "columns_to_save = [\n",
    "    'sample_id', \n",
    "    'item_name', \n",
    "    'price', \n",
    "    'predicted_price', \n",
    "    'smape_score'\n",
    "]\n",
    "\n",
    "# Save the selected columns of the top 1000 worst samples to a CSV file\n",
    "top_1000_worst[columns_to_save].to_csv('worst_performing_samples.csv', index=False)\n",
    "\n",
    "print(\"✅ Successfully saved the top 1000 worst predictions to 'worst_performing_samples.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730fc6b-5a67-4f25-848b-fa177fba91c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
