{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WSiKNFfMfO9"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# FULL PIPELINE: LGBM + optional CatBoost + SMAPE FEVAL\n",
        "# ===============================\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ------------- USER PARAMS (edit if needed) -------------\n",
        "TRAIN_PATH = '/mnt/sagemaker-nvme/feature_merge/train_CLEAN_FINAL.csv'\n",
        "TEST_PATH  = '/mnt/sagemaker-nvme/feature_merge/test_CLEAN_FINAL.csv'\n",
        "TARGET_COL = 'price'            # final ground-truth column name (or 'log_price' if only that exists)\n",
        "ID_COL     = 'id'               # if your test has a different id name, change it\n",
        "N_SPLITS   = 3                  # CV folds (set 3 for speed; set 5 if you have time)\n",
        "TOP_K      = 1200               # keep top-K features by importance (reduce if memory issues)\n",
        "SEEDS      = [42, 2024]         # seeds for seed-ensemble, can add more\n",
        "USE_CATBOOST = True             # try CatBoost blending if available\n",
        "RANDOM_STATE = 42\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def clean_column_names(df):\n",
        "    \"\"\"Remove special JSON characters and ensure unique names.\"\"\"\n",
        "    new_names = {col: re.sub(r'[^A-Za-z0-9_]+','', col) for col in df.columns}\n",
        "    seen = {}\n",
        "    final = {}\n",
        "    for old, new in new_names.items():\n",
        "        if new == '': new = 'col'\n",
        "        if new in seen:\n",
        "            seen[new] += 1\n",
        "            new2 = f\"{new}_{seen[new]}\"\n",
        "            final[old] = new2\n",
        "        else:\n",
        "            seen[new] = 0\n",
        "            final[old] = new\n",
        "    return df.rename(columns=final)\n",
        "\n",
        "def smape_true(y_true, y_pred):\n",
        "    \"\"\"SMAPE on original scale (in percent).\"\"\"\n",
        "    num = np.abs(y_pred - y_true)\n",
        "    den = (np.abs(y_true) + np.abs(y_pred)) + 1e-8\n",
        "    return np.mean(200.0 * num / den)\n",
        "\n",
        "def seed_print(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "# ---------- Load data (with dtype reduction) ----------\n",
        "seed_print(\"Loading data...\")\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "# seed_print(train.info())\n",
        "\n",
        "seed_print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
        "\n",
        "# ---------- Target detection ----------\n",
        "if TARGET_COL in train.columns:\n",
        "    y = train[TARGET_COL].astype('float32')\n",
        "elif 'log_price' in train.columns:\n",
        "    # recover if only log exists\n",
        "    y = np.expm1(train['log_price'].astype('float32'))\n",
        "else:\n",
        "    raise ValueError(\"No target column found. Change TARGET_COL or ensure 'price'/'log_price' present.\")\n",
        "\n",
        "# ---------- Prepare features ----------\n",
        "X = train.drop(columns=[c for c in ['price','log_price'] if c in train.columns], errors='ignore').copy()\n",
        "X_test = test.copy()\n",
        "\n",
        "# ---------- Clean names and align ----------\n",
        "seed_print(\"Cleaning column names and aligning train/test...\")\n",
        "X = clean_column_names(X)\n",
        "X_test = clean_column_names(X_test)\n",
        "\n",
        "# Ensure same order and same features\n",
        "common = [c for c in X.columns if c in X_test.columns]\n",
        "X = X[common].copy()\n",
        "X_test = X_test[common].copy()\n",
        "seed_print(f\"Features count after align: {len(common)}\")\n",
        "\n",
        "# ---------- Reduce memory (float32 etc) ----------\n",
        "seed_print(\"Converting dtypes to save memory...\")\n",
        "for c in X.select_dtypes(include=['float64']).columns:\n",
        "    X[c] = X[c].astype('float32')\n",
        "    X_test[c] = X_test[c].astype('float32')\n",
        "for c in X.select_dtypes(include=['int64']).columns:\n",
        "    X[c] = X[c].astype('int32')\n",
        "    X_test[c] = X_test[c].astype('int32')\n",
        "\n",
        "# ==========================\n",
        "# Detect and Convert Categorical Columns Safely\n",
        "# ==========================\n",
        "cat_cols = []\n",
        "\n",
        "for c in X.columns:\n",
        "    # only treat as categorical if object type or low-cardinality\n",
        "    if X[c].dtype == 'object' or X[c].nunique() <= 100:\n",
        "        # skip numeric columns\n",
        "        if X[c].dtype not in ['float64', 'int64']:\n",
        "            X[c] = X[c].astype('category')\n",
        "            X_test[c] = X_test[c].astype('category')\n",
        "            cat_cols.append(c)\n",
        "\n",
        "# ---------------------------\n",
        "# ENSURE NO CATEGORICAL FEATURES (force numeric-only for LGBM)\n",
        "# ---------------------------\n",
        "seed_print(\"Ensuring no categorical dtypes and disabling categorical_feature for LGBM...\")\n",
        "\n",
        "# 1) If any column accidentally has pandas 'category' dtype, convert it back.\n",
        "#    Try to restore numeric values; if not possible, use category codes (safe integer).\n",
        "for col in X.select_dtypes(include=['category']).columns.tolist():\n",
        "    seed_print(f\"Reverting category dtype for column: {col}\")\n",
        "    try:\n",
        "        # If categories are numeric-like, this will restore numbers\n",
        "        X[col] = pd.to_numeric(X[col], errors='raise')\n",
        "        X_test[col] = pd.to_numeric(X_test[col], errors='raise')\n",
        "    except Exception:\n",
        "        # fallback: use safe integer codes (0..n-1), but we prefer to not treat them as categorical\n",
        "        X[col] = X[col].cat.codes.astype('int32')\n",
        "        X_test[col] = X_test[col].cat.codes.astype('int32')\n",
        "\n",
        "# 2) Remove any accidentally-created 'object' dtypes that are actually numeric strings\n",
        "#    (optional but helpful). Try to coerce object cols to numeric where possible.\n",
        "for col in X.select_dtypes(include=['object']).columns.tolist():\n",
        "    try:\n",
        "        X[col] = pd.to_numeric(X[col], errors='raise')\n",
        "        X_test[col] = pd.to_numeric(X_test[col], errors='raise')\n",
        "        seed_print(f\"Coerced object->numeric for {col}\")\n",
        "    except Exception:\n",
        "        # if it really is stringy, leave as object but we WILL NOT pass as categorical to LGBM\n",
        "        seed_print(f\"Left as object (non-numeric) column: {col}\")\n",
        "\n",
        "# 3) Now explicitly clear any cat_cols list and ensure we never pass categorical_feature to LGBM\n",
        "cat_cols = []   # <- key: ensure empty\n",
        "\n",
        "# 4) Double-check dtypes summary (for debugging / quick assertion)\n",
        "seed_print(\"Final dtypes summary (train):\")\n",
        "print(X.dtypes.value_counts())\n",
        "seed_print(\"If you see 'category' still, inspect columns above and convert them explicitly.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "522iQPfLMgjV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Quick feature importance sampling to get top-K features ----------\n",
        "seed_print(\"Running small sample LGB for feature importance (to pick top-K)...\")\n",
        "SAMPLE_N = min(12000, len(X))\n",
        "sample_idx = np.random.RandomState(RANDOM_STATE).choice(len(X), SAMPLE_N, replace=False)\n",
        "X_s = X.iloc[sample_idx].copy()\n",
        "y_s = np.log1p(y.iloc[sample_idx])  # importance training on log-target\n",
        "dtrain_s = lgb.Dataset(X_s, label=y_s, categorical_feature=None)\n",
        "params_small = {\n",
        "    'objective': 'regression_l1', 'metric': 'mae',\n",
        "    'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 500,\n",
        "    'verbose': -1, 'max_bin': 127, 'n_jobs': 4, 'min_data_in_leaf': 20\n",
        "}\n",
        "bst_small = lgb.train(params_small, dtrain_s, num_boost_round=300)\n",
        "importances = pd.DataFrame({\n",
        "    'feature': X_s.columns,\n",
        "    'imp_gain': bst_small.feature_importance(importance_type='gain'),\n",
        "    'imp_split': bst_small.feature_importance(importance_type='split')\n",
        "}).sort_values('imp_gain', ascending=False)\n",
        "\n",
        "TOP_K = min(TOP_K, len(importances))\n",
        "top_feats = importances['feature'].values[:TOP_K].tolist()\n",
        "seed_print(f\"Keeping top {TOP_K} features (by gain).\")\n",
        "del X_s, y_s, dtrain_s, bst_small, importances; gc.collect()\n",
        "\n",
        "X = X[top_feats].copy()\n",
        "X_test = X_test[top_feats].copy()\n",
        "\n",
        "# ---------- Prepare log1p target for training ----------\n",
        "y_log = np.log1p(y.values)\n",
        "\n",
        "# ---------- Define SMAPE FEVAL for LightGBM (IMPORTANT) ----------\n",
        "def lgb_smape_eval(preds_log, train_data):\n",
        "    \"\"\"\n",
        "    LightGBM custom eval: preds and labels are in log1p space.\n",
        "    We must convert back to original scale for SMAPE calculation.\n",
        "    \"\"\"\n",
        "    labels_log = train_data.get_label()\n",
        "    # inverse transform to original space\n",
        "    preds = np.expm1(preds_log)\n",
        "    labels = np.expm1(labels_log)\n",
        "    # compute SMAPE (not percent but mean percent; we keep percent)\n",
        "    num = np.abs(preds - labels)\n",
        "    den = (np.abs(preds) + np.abs(labels)) + 1e-8\n",
        "    smape = np.mean(200.0 * num / den)\n",
        "    return 'SMAPE', smape, False  # False -> lower is better\n",
        "\n",
        "# ---------- LightGBM params (CPU safe) ----------\n",
        "lgb_params = {\n",
        "    'objective': 'regression_l1',\n",
        "    'metric': 'mae',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.02,\n",
        "    'num_leaves': 64,\n",
        "    'feature_fraction': 0.6,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 5,\n",
        "    'min_data_in_leaf': 30,\n",
        "    'max_bin': 127,\n",
        "    'lambda_l1': 0.5,\n",
        "    'lambda_l2': 1.0,\n",
        "    'n_jobs': 4,\n",
        "    'verbose': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# ---------- Function to run CV for one seed ----------\n",
        "def run_lgb_cv(X, X_test, y_log, params, n_splits=3, seed=42):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    oof_log = np.zeros(len(X))\n",
        "    test_log = np.zeros(len(X_test))\n",
        "    fold_metrics = []\n",
        "    start = time.time()\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"--- Seed {seed} | Fold {fold+1}/{n_splits} ---\")\n",
        "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y_log[tr_idx], y_log[val_idx]\n",
        "\n",
        "        lgb_tr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=None)\n",
        "        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_tr, categorical_feature=None)\n",
        "\n",
        "        bst = lgb.train(\n",
        "            params,\n",
        "            lgb_tr,\n",
        "            num_boost_round=3000,\n",
        "            valid_sets=[lgb_val],\n",
        "            feval=lgb_smape_eval,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(150),\n",
        "                lgb.log_evaluation(500)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        oof_log[val_idx] = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
        "        test_log += bst.predict(X_test, num_iteration=bst.best_iteration) / n_splits\n",
        "\n",
        "        # evaluate SMAPE on original scale for this fold\n",
        "        y_val_orig = np.expm1(y_val)\n",
        "        oof_val_orig = np.expm1(oof_log[val_idx])\n",
        "        sm = smape_true(y_val_orig, oof_val_orig)\n",
        "        fold_metrics.append(sm)\n",
        "        print(f\"Fold {fold+1} SMAPE (orig scale): {sm:.6f}\")\n",
        "\n",
        "        # cleanup\n",
        "        del X_tr, X_val, y_tr, y_val, lgb_tr, lgb_val, bst\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"[seed {seed}] CV SMAPE mean/std: {np.mean(fold_metrics):.6f} Â± {np.std(fold_metrics):.6f}\")\n",
        "    print(\"Elapsed (mins):\", (time.time()-start)/60.0)\n",
        "    return oof_log, test_log, fold_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0MXQqAzMgT9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Run LightGBM for multiple seeds (seed ensemble) ----------\n",
        "seed_print(\"Starting LightGBM training for seeds: \" + str(SEEDS))\n",
        "oof_stack = np.zeros((len(X), len(SEEDS)))\n",
        "test_stack = np.zeros((len(X_test), len(SEEDS)))\n",
        "seed_metrics = []\n",
        "\n",
        "for i, sd in enumerate(SEEDS):\n",
        "    params = lgb_params.copy()\n",
        "    params['seed'] = sd\n",
        "    oof_log, test_log, fmetrics = run_lgb_cv(X, X_test, y_log, params, n_splits=N_SPLITS, seed=sd)\n",
        "    oof_stack[:, i] = oof_log\n",
        "    test_stack[:, i] = test_log\n",
        "    seed_metrics.append((np.mean(fmetrics), np.std(fmetrics)))\n",
        "\n",
        "# Convert ensemble logs -> original scale by expm1 and average\n",
        "seed_print(\"Converting log preds to original scale and averaging seeds...\")\n",
        "oof_ens = np.expm1(oof_stack).mean(axis=1)\n",
        "test_ens = np.expm1(test_stack).mean(axis=1)\n",
        "\n",
        "overall_cv_smape = smape_true(np.expm1(y_log), oof_ens)\n",
        "seed_print(f\"Ensemble CV SMAPE (all seeds) = {overall_cv_smape:.6f}\")\n",
        "\n",
        "# ---------- Optional: CatBoost quick blend (if available) ----------\n",
        "if USE_CATBOOST:\n",
        "    try:\n",
        "        from catboost import CatBoostRegressor\n",
        "        seed_print(\"CatBoost found. Running quick CatBoost (single seed) on TOP_K features.\")\n",
        "        # convert cat columns indices for CatBoost\n",
        "        cat_idx = [i for i, c in enumerate(X.columns) if c in cat_cols]\n",
        "        oof_cb = np.zeros(len(X))\n",
        "        test_cb = np.zeros(len(X_test))\n",
        "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEEDS[0])\n",
        "        cb_scores = []\n",
        "\n",
        "        for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "            print(f\"CatBoost Fold {fold+1}/{N_SPLITS}\")\n",
        "            X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "            y_tr, y_val = y_log[tr_idx], y_log[val_idx]\n",
        "\n",
        "            model_cb = CatBoostRegressor(\n",
        "                iterations=2000, learning_rate=0.03, depth=6,\n",
        "                loss_function='MAE', eval_metric='MAE',\n",
        "                early_stopping_rounds=150, verbose=500, random_seed=SEEDS[0]\n",
        "            )\n",
        "            model_cb.fit(X_tr, y_tr, eval_set=(X_val, y_val), cat_features=cat_idx)\n",
        "            oof_cb[val_idx] = model_cb.predict(X_val)\n",
        "            test_cb += model_cb.predict(X_test) / N_SPLITS\n",
        "\n",
        "            # fold SMAPE on original scale\n",
        "            cb_fold_smape = smape_true(np.expm1(y_val), np.expm1(oof_cb[val_idx]))\n",
        "            cb_scores.append(cb_fold_smape)\n",
        "            print(\"CatBoost Fold SMAPE:\", cb_fold_smape)\n",
        "\n",
        "        print(\"CatBoost CV SMAPE:\", np.mean(cb_scores), np.std(cb_scores))\n",
        "        # Blend LightGBM ensemble and CatBoost (adjust weights as needed)\n",
        "        blend_weight_lgb = 0.65\n",
        "        blend_weight_cb  = 0.35\n",
        "        final_test_pred = blend_weight_lgb * test_ens + blend_weight_cb * np.expm1(test_cb)\n",
        "        final_oof = blend_weight_lgb * oof_ens + blend_weight_cb * np.expm1(oof_cb)\n",
        "        seed_print(f\"Blended CV SMAPE = {smape_true(np.expm1(y_log), final_oof):.6f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        seed_print(\"CatBoost not available or failed. Skipping. Error:\")\n",
        "        print(e)\n",
        "        final_test_pred = test_ens\n",
        "        final_oof = oof_ens\n",
        "else:\n",
        "    final_test_pred = test_ens\n",
        "    final_oof = oof_ens\n",
        "\n",
        "# ---------- Clip predictions to safe range ----------\n",
        "min_price = 0.0\n",
        "max_price = float(np.percentile(np.expm1(y_log), 99.99) * 3.0)\n",
        "final_test_pred = np.clip(final_test_pred, min_price, max_price)\n",
        "\n",
        "# ---------- Final evaluation on CV OOF ----------\n",
        "cv_smape_final = smape_true(np.expm1(y_log), final_oof)\n",
        "seed_print(f\"Final OOF SMAPE (orig scale): {cv_smape_final:.6f}\")\n",
        "\n",
        "# ---------- Prepare submission ----------\n",
        "seed_print(\"Preparing submission CSV...\")\n",
        "if ID_COL in test.columns:\n",
        "    sub = pd.DataFrame({ID_COL: test[ID_COL], 'price': final_test_pred})\n",
        "else:\n",
        "    sub = pd.DataFrame({'id': np.arange(len(test)), 'price': final_test_pred})\n",
        "\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "seed_print(f\"Saved submission to {out_path}\")\n",
        "\n",
        "# ---------- Done ----------\n",
        "seed_print(\"Pipeline complete. If CV SMAPE is high, try: increasing TOP_K, adding seeds, training CatBoost, or stacking.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
